[ { "title": "Using NGINX as API Gateway", "url": "/posts/using-nginx-as-api-gateway/", "categories": "microservices", "tags": "nginx, devops, api gateway, microservices", "date": "2022-07-08 15:00:00 -0300", "snippet": "Hi everyone!If you ever need to deploy a reverse proxy, you may have heard of NGINX (engine x). In case you haven’t heard it yet, let’s talk a little about it here and how we can use it as an API Gateway.What is NGINX?NGINX is an HTTP server and reverse proxy, a mail proxy server, and a generic TCP/UDP proxy server.According to Netcraft, NGINX served or proxied 21.67% of the busiest sites in May 2022.Currently, NGINX has an open-source version and a commercial version (NGINX Plus). In this post, we will try to use some of the features of the free version but also available in the paid version.What is an API Gateway?An API Gateway is a traffic manager that allows developers to create, publish, maintain, monitor, and secure APIs. In resume, it interfaces between external traffic and backend services.Using an API Gateway has several advantages, such as: Make APIs more secure through a single interface. Enable you to more easily enforce access control policies, rate limits, routing, mediation, etc. Enables the most comprehensive collection of metricsThat said, let’s put some concepts into practice!What do we want?Let’s say we have two APIs: a for products, which we call the Products API, and other for users, which we call the Users API. Initially, we will publish our APIs. Our architecture diagram starts like this:We want to create the following routes: /api/products -&gt; should point to the Products API service /api/users -&gt; should point to the Users API serviceA relevant feature in our settings is that we don’t want to expose our services publicly. Therefore, our only gateway should be the NGINX which we call API Gateway in the diagram above.To simulate our API, we built two simple Python applications with fixed responses (Products API and Users API).For that, let’s create a simple rule in our NGINX configuration file (gateway.conf):server { listen 80 default_server; listen [::]:80 default_server; # # Products API # location /api/products { proxy_pass http://products_api:8001; } # # Users API # location /api/users { proxy_pass http://users_api:8002; }}In the configuration above, notice that I created an alias for the API containers (see docker-compose.yml - contains spoilers).So, the expected answers are:GET http://localhost/api/products{ \"name\": \"Product 1\", \"description\": \"Detail about product 1\"}GET http://localhost/api/users{ \"name\": \"User 1\", \"email\": \"email@email.com\"}In this way, we have NGINX serving as a reverse proxy. To improve the organization of our configurations we will define upstream for our API instead of directly using their address (alias). So we will have:upstream products_api_server { server products_api:8001;} upstream users_api_server { server users_api:8002;}And after:server { listen 80 default_server; listen [::]:80 default_server; # # Products API # location /api/products { proxy_pass http://products_api_server; } # # Users API # location /api/users { proxy_pass http://users_api_server; }}The upstream (ngx_http_upstream_module) is used to define groups of servers that can be referenced by the proxy_pass, fastcgi_pass, uwsgi_pass, scgi_pass, memcached_pass, and grpc_pass directives.After applying our basic settings, let’s increment the Gateway.Load BalanceIn the next scenario, let’s assume that our Users API got overloaded and we decided to add a new instance (users_api_balance) to receive requests from end-users.We will configure our API Gateway to do this balancing. For that, we’ll add the new server to the Users API server group.upstream products_api_server { server products_api:8001;} upstream users_api_server { server users_api:8002; server users_api_balance:8002;}Through the container logs, you can see that the requests are distributed among the servers.api-gateway-nginx | 172.18.0.1 - - [07/Jul/2022:22:52:49 +0000] \"GET /api/users HTTP/1.1\" 200 46container_users_api | 172.18.0.5 - - [07/Jul/2022 22:52:49] \"GET /api/users HTTP/1.0\" 200api-gateway-nginx | 172.18.0.1 - - [07/Jul/2022:22:52:49 +0000] \"GET /api/users HTTP/1.1\" 200 46container_users_api_balance | 172.18.0.5 - - [07/Jul/2022 22:52:50] \"GET /api/users HTTP/1.0\" 200With these settings, the requests are distributed evenly (Round Robin) between servers. But how about we evolve our balancing rules? Now let’s define that we should direct the new request to the server with the lowest number of active connections. Again, we will only modify the Users API server group by passing the desired balancing method. In our case, it will be the least_conn.upstream products_api_server { server products_api:8001;} upstream users_api_server {least_conn; server users_api:8002; server users_api_balance:8002;}We’ll use this setup for now, but NGINX has a few other balancing methods: ip_hash: Generic Hash Random Least Time (NGINX Plus only)More details in the official documentation.However, let’s assume tha the users_api server has a better infrastructure, and we want to prioritize requests to this server. We can do this through the weight parameter.upstream products_api_server { server products_api:8001;} upstream users_api_server {least_conn; server users_api:8002 weight=5; server users_api_balance:8002;}With that, the users_api server has a priority five times higher than the other servers.CacheIn this scenario, let’s assume that the Products API is not volatile, and we want to optimize for response time. For this, we will add a basic cache in API Gateway,Only two directives are needed to enable basic caching: proxy_cache_path and proxy_cache. The proxy_cache_path directive sets the path and configuration of the cache, and the proxy_cache directive activates it.upstream products_api_server { server products_api:8001;} upstream users_api_server { ip_hash; server users_api:8002; server users_api_balance:8002;} proxy_cache_path /tmp/products levels=1:2 keys_zone=products_cache:10m max_size=10g inactive=60m use_temp_path=off; server { listen 80 default_server; listen [::]:80 default_server; # # Products API # location /api/products { proxy_cache products_cache; proxy_pass http://products_api_server; } # # Users API # location /api/users { proxy_pass http://users_api_server; }}The parameters to the proxy_cache_path directive define the following settings: The local disk directory for the cache is called /tmp/products. levels sets up a two‑level directory hierarchy under /tmp/products. If the levels parameter is not included, NGINX puts all files in the same directory. keys_zone sets up a shared memory zone for storing the cache keys and metadata such as usage timers. max_size sets the upper limit of the size of the cache (to 10 gigabytes in this example). It is optional; not specifying a value allows the cache to grow to use all available disk space. inactive specifies how long an item can remain in the cache without being accessed. In this example, a file that has not been requested for 60 minutes is automatically deleted from the cache by the cache manager process, regardless of whether or not it has expired. The default value is 10 minutes (10m). NGINX first writes files that are destined for the cache to a temporary storage area, and the use_temp_path=off directive instructs NGINX to write them to the same directories where they will be cached.The default form of the keys that NGINX generates is similar to an MD5 hash of the following NGINX variables: $scheme$proxy_host$request_uri; the actual algorithm used is slightly more complicated.More details on caching can be seen on the NGINX Blog: A Guide to Caching with NGINX and NGINX Plus.Rate LimitHaving defined our balancing and cache rules, we can now protect our internal services from a large number of requests (DDoS attacks) by setting a rate limit for consumers.Rate limiting is configured with two main directives, limit_req_zone and limit_req. The limit_req_zone directive defines the parameters for rate limiting while limit_req enables rate limiting within the context where it appears.upstream products_api_server { server products_api:8001;} upstream users_api_server { ip_hash; server users_api:8002; server users_api_balance:8002;} proxy_cache_path /tmp/products levels=1:2 keys_zone=products_cache:10m max_size=10g inactive=60m use_temp_path=off;limit_req_zone $binary_remote_addr zone=products_rate:10m rate=1r/s;limit_req_zone $binary_remote_addr zone=user_rate:10m rate=10r/s; server { listen 80 default_server; listen [::]:80 default_server; # # Products API # location /api/products { proxy_cache products_cache; limit_req zone=products_rate; limit_req_status 429; proxy_pass http://products_api_server; } # # Users API # location /api/users { limit_req zone=user_rate; limit_req_status 429; proxy_pass http://users_api_server; }}limit_req_zone takes the following three parameters: Key – Defines the request characteristic against which the limit is applied. In the example it is the NGINX variable $binary_remote_addr, which holds a binary representation of a client’s IP address. This means we are limiting each unique IP address to the request rate defined by the third parameter. Zone – Defines the shared memory zone used to store the state of each IP address and how often it has accessed a request‑limited URL. Keeping the information in shared memory means it can be shared among the NGINX worker processes. Rate – Sets the maximum request rate. In the example, the rate cannot exceed 10 request per second for Users API and 1 requests per second for Products API.By default, NGINX will return 503 (Service Temporarily Unavailable) when the request limit is reached. To improve this, we use limit_req_status to customize the response status code. In this case we use 429 (Too Many Requests).More details on using rate limit can be seen on the NGINX Blog: Rate Limiting with NGINX and NGINX PlusAPI Key AuthenticationIt is unusual to publish APIs without some form of authentication to protect them. NGINX offers several approaches for protecting APIs and authenticating API clients. In our solution, we will use a simple solution to validate access to our services. We will use API Keys Authentication.With API key authentication, we use a map block to create an allowlist of client names that can access our services.map $http_apikey $api_client_name { default \"\"; \"KrtKNkLNGcwKQ56la4jcHwxF\" \"client_one\"; \"sqj3Ye0vFW/CM/o7LTSMEMM+\" \"client_two\"; \"diXnbzglAWMMIvyEEV3rq7Kt\" \"client_ten\";}The map directive takes two parameters. The first defines where to find the API key, in this case in the apikey HTTP header of the client request as captured in the $http_apikey variable. The second parameter creates a new variable ($api_client_name) and sets it to the value of the second parameter on the line where the first parameter matches the key.Now enable API Key authentication on our services. Just to avoid code duplication I will separate the API Key validation into another method.# API key validationlocation = /_validate_apikey { internal; if ($http_apikey = \"\") { return 401; # Unauthorized } if ($api_client_name = \"\") { return 403; # Forbidden } return 204; # OK (no content)}## Products API#location /api/products { auth_request /_validate_apikey; proxy_cache products_cache; limit_req zone=products_rate; limit_req_status 429; proxy_pass http://products_api_server;}## Users API#location /api/users { auth_request /_validate_apikey; limit_req zone=user_rate; limit_req_status 429; proxy_pass http://users_api_server;}With this configuration in place, the Products API and Users APIs now implements API key authentication.curl -I 'http://localhost/api/products' --header 'apikey: INVALID_KEY'HTTP/1.1 403 ForbiddenServer: nginx/1.21.5Date: Sat, 09 Jul 2022 02:24:03 GMTContent-Type: text/htmlContent-Length: 153Connection: keep-alivecurl -I 'http://localhost/api/products' --header 'apikey: diXnbzglAWMMIvyEEV3rq7Kt'HTTP/1.1 200 OKServer: nginx/1.21.5Date: Sat, 09 Jul 2022 02:25:03 GMTContent-Type: text/html; charset=utf-8Content-Length: 62Connection: keep-aliveNote that the above solution is quite limited, but it is possible to authenticate using JWT. However, this functionality is natively only available in NGINX Plus. For NGINX open source, an introspection endpoint is required to perform the validation.Other more robust solutions are using that can be used for authentication, are: OAuth Proxy Module or Phantom Token Module.ConclusionFinally, our final architecture is shown in the diagram below. Here you can find all the codes we developed.In addition to the features presented in this article, NGINX offers several others (especially in NGINX Plus). Several of them can be found in the free ebook or on the tool’s official blog.Well, that’s it, folks. I hope you enjoyed the article. See you soon!" }, { "title": "Scaffolding for the golang project", "url": "/posts/golang-scaffolding/", "categories": "golang", "tags": "golang, getting started, scaffolding, uber-fx, go-chi", "date": "2022-07-04 13:00:00 -0300", "snippet": "Hello guys! We’re back!As I did for projects using Django I created scaffolding for projects in Golang.In the scaffolding project I used some dependencies: go-chi as the HTTP router. uber-fx for dependency injection.Also, I tried using the standard project layout described here.Project repositoryFor the guys with suggestions for improvements, just send a PR or create an issue in the project.See you later guys! :)" }, { "title": "Getting Started with Django 2", "url": "/posts/django-getting-started/", "categories": "python", "tags": "python, django, getting started", "date": "2018-01-11 14:00:00 -0200", "snippet": "Eai pessoal! Disponibilizei no meu GitHub um simples projeto para a galera que está iniciando/testando o Django (versão 2.0).Basicamente, é uma pequena variação do Getting Started disponibilizado no site oficial do Django.O projeto é uma simples aplicação de categorização de filmes (chamada de Flix). Nele resolvi utilizar GenericViews, organizar os pacotes da forma que geralmente faço e adicionar o Bootstrap 4. Em breve, disponibilizarei no mesmo projeto testes utilizando o pacote de testes do próprio Django(django.test).Repositório do projetoPara a galera com sugestões de melhorias, basta enviar um PR ou criar uma issue no projeto.Abraços :)" }, { "title": "A Carreira de Desenvolvedor: Do Jr ao Sênior", "url": "/posts/carreira-de-desenvolvedor/", "categories": "general", "tags": "general, personal, career", "date": "2017-11-11 02:00:00 -0200", "snippet": "Eai pessoal! Essa semana tive a oportunidade de compartilhar um pouco das minhas experiências profissionais com os alunos do Instituto Federal de Santa Cantarina (Campus Canoinhas) no I Workshop de Informática, que aconteceu nos dias 09 e 10 de novembro.Com a palestra intitulada “A Carreira de Desenvolvedor: do Jr ao Sênior”, tentei compartilhar dicas e recursos que podem auxiliar no desenvolvimento das habilidades técnicas que o mercado atual exige.A apresentação está disponível no SlideShare (acesse aqui).Abraços :)" }, { "title": "Diário de uma Mudança: Parte 3 e 4 – Primeira Semana em Floripa e Lar Doce Lar", "url": "/posts/diario-de-uma-mudanca-parte-3/", "categories": "general", "tags": "general, personal", "date": "2017-10-13 14:00:00 -0300", "snippet": "Eai pessoal! Depois de mais de um mês ausente, estou de volta com a série de postagens sobre a minha mudança de emprego/cidade.Para não estender mais essa série, resolvi unificar as duas últimas partes. Sendo assim, nessa postagem irei falar sobre a primeira semana em Floripa, a primeira semana de trabalho na Agriness e a batalha para encontrar um lugar para morar.Primeira Semana em FloripaA primeira semana em Florianópolis foi relativamente tranquila. Desembarquei na cidade em um sábado e me apresentei na empresa na segunda-feira seguinte. Nessa primeira semana aproveitei para visitar vários apartamentos, conhecer a cidade e me atualizar sobre os desafios que enfrentaria no novo trabalho.Com relação a cidade, fiquei realmente encantado. Desde o clima agradável à educação dos moradores (pelo menos os que interagi :D). A cidade é turística, sendo assim existem várias opções de restaurantes, baladas e esportes - para todos os gostos.Aqui a galera parece ser aficionada por esportes. São bem comuns grupos de trilhas, academias lotadas, gente correndo na orla, ciclovias ocupadas e etc. Por outro lado, existe uma infinidade de tipos/marcas de cerveja. A cidade parece ser cercada por fábricas – além das produções artesanais.Entretanto, também notei alguns pontos negativos. Os principais foram: o transporte público parece ser ineficiente e o trânsito tem como principal gargalo a ponte que liga a ilha ao continente. Felizmente, para minimizar esses dois problemas, o Uber está presente e funcionando perfeitamente.Primeira Semana de TrabalhoComo havia dito, iniciei meus trabalhos praticamente dois dias após chegar na cidade. A primeira semana foi basicamente de estudos. Fui apresentado aos processos e metodologias utilizados na empresa e comecei o Programa de Integração Garinova (PIG).No PIG, foram apresentados todos os setores da empresa. Tive a oportunidade de conhecer todas as equipes e as funções de cada uma delas. Além disso, tive um breve curso sobre suinocultura, principal área de atuação da Agriness.Quanto a parte técnica, fiquei bastante contente ao ver processos definidos, equipe engajada e competente, e um produto desafiador sendo (re)estruturado. Ainda existe um longo caminho pela frente, mas estamos aqui para isso. \\o/(Em postagens futuras irei falar um pouco sobre as tecnologias utilizadas)Lar doce LarO maior desafio que enfrentei no primeiro mês foi o de encontrar um apartamento que fosse perto do trabalho, com algumas mobílias e cujo custo coubesse no bolso.Visitei vários apartamentos e gostei de muitos deles, mas parecia que todo mundo estava de mudança para Florianópolis! Praticamente todos estavam com fila de reserva e quando encontrava um livre, a imobiliária exigia uma infinidade de documentos, meus e de meus pais (parte desses documentos tinha que vir de Maceió, ou seja, mais demora!).Felizmente, no fim encontrei um apartamento a cerca de 600 metros do trabalho (uma caminhada super agradável de menos de 10 minutos! :D ). Entretanto, o lado ruim é que ele estava vazio. Então, tive que arcar com a compra de vários móveis, eletros e itens de casa não previsto.Bom pessoal! É isso!Abraços" }, { "title": "Diário de uma Mudança: Parte 2 - Passagem Só de Ida", "url": "/posts/diario-de-uma-mudanca-parte-2/", "categories": "general", "tags": "general, personal", "date": "2017-08-11 00:02:11 -0300", "snippet": "Dando continuidade a história do processo de mudança de cidade/emprego que venho passando, na postagem dessa semana irei compartilhar com vocês como foi a etapa de saída de Maceió/AL. Compartilharei como uma reorganização financeira antecipada e a escolha da empresa certa facilitaram essa etapa.Vamos lá!Reorganização financeira – preparado para mudarQuando aceitei o convite de trabalho em Florianópolis, também fui questionado como conseguiria me mudar definitivamente em tão pouco tempo, considerando que teria menos de duas semanas para me apresentar no novo emprego. Bom, a resposta para essa questão é: já estava preparado para mudar. Para explicar melhor esse ponto é preciso falar um pouco como foi meu retorno ao mercado alagoano – sem me estender muito.Durante quatros anos trabalhei para uma empresa de São Paulo, onde fui engenheiro de software e líder da equipe de desenvolvimento. Quando precisei retornar para uma empresa de Alagoas, o primeiro impacto foi a grande diferença salarial, mesmo usando como ponderador o custo de vida das cidades/estados em questão.(Nesse ponto da postagem, convido vocês a realizarem um comparativo de sua atual remuneração com o valor médio pago em sua e outras cidade. Para isso, ferramentas como Glassdor, LoveMondays ou a Calculadora de Salários da GeekHunter podem ajudar.)De início acreditei que pudesse ser algo momentâneo, que com o passar do tempo a confiança seria adquirida e o reconhecimento apareceria. Entretanto, percebi que estava enganado (ou não tive paciência para esperar o suficiente – foram quase dois anos). Foi durante esse tempo que comecei a aceitar que dificilmente conseguiria o que procurava na minha cidade natal. Então, estar pronto para uma possível mudança de cidade passou a fazer parte dos meus planos. Foi aí que dei início a uma reorganização financeira.Em resumo, foram pouco mais de doze meses de reorganização para uma mudança que poderia acontecer a qualquer momento, bastava surgir a oportunidade certa.Entre as várias ações necessárias, boa parte visava enxugar despesas. Cito como principais três: Retorno para casa dos meus pais: após obter uma certa estabilidade financeira, uma das primeiras coisas que fiz foi buscar meu espaço. Um lugar em que pudesse construir as coisas do jeito que queria. Então, saí da casa dos meus pais e aluguei um apartamento. Entretanto, quando sair da minha cidade natal entrou nos meus planos, percebi que morar sozinho envolvia custos que tive que reconsiderar. Então, larguei meu espaço e retornei para casa dos meus pais – decisão difícil, mas necessária. (Apenas para deixar claro, retornar para casa dos meus pais foi um ótimo acontecimento. O “sacrifício” aqui, está relacionado em abandonar o lugar que já considerava minha casa.) Corte de custos: para esse caso, foram necessários alguns meses de reestruturação. O corte de custos com o apartamento já foi ótimo, mas percebi que podia enxugar muito mais despesas. Sendo assim, comecei a cancelar/reduzir vários serviços (alguns que nem lembrava que assinava!). Exemplos: cancelamento Dropbox, cancelamento do 500px, cancelamento do AWS (passei a usar a DigitalOcean), cancelamento de cursos online que nunca acabava, redução do plano de celular (cheguei a pagar absurdos R$ 350 por mês!!!), mudança de academia, adeus iPhone novo, entre outros. Somente com essa restruturação, consegui reduzir os custos mensais em mais de 25% - pode parecer pouco, mas faz diferença no final. Não investimento em coisas que me prendessem a Maceió: esse ponto foi relativamente fácil. Apenas evitei investimentos no que pudessem pesar na decisão de sair ou não de Maceió. Exemplos: compra de imóveis e contratos de longo prazo. Com essas medidas, depois de alguns meses estava pronto para concorrer e aceitar propostas em outras cidades sem correr o risco de ter um descontrole financeiro.A empresa certaComo falei na Parte 1 dessa sequência de postagens, levei em consideração vários fatores na hora de aceitar participar de processos seletivos. Não acredito que “me dei ao luxo” de escolher, apenas analisei os fatores que poderiam influenciar no desempenho de minhas atividades profissionais e/ou impactar negativamente na vida pessoal. Dentre os pontos que levei em consideração, destaco dois: Qualidade de vida da cidade: inicialmente acreditava que morar em uma cidade cuja qualidade de vida não fosse boa, seria um sacrifício pessoal recompensando no lado profissional. Entretanto, percebi que esse era um fator de extrema importância. Morar em uma cidade com graves problemas de segurança, infraestrutura e/ou econômico, certamente afetaria meu desempenho profissional e vida pessoal. Logo, era um risco muito alto não considerar esse item. Perfil da empresa: esse foi o fator mais relevante que considerei durante a busca pelo novo emprego. Acredito que quando o perfil da empresa se encaixa com o seu, a realização das atividades flui mais facilmente e sem desgastes. Por isso, ao meu ver era importante saber que participaria de uma equipe integrada, focada e sem muitos vícios. Em outras palavras, buscava uma empresa onde a meta estivesse clara, a equipe estivesse empenhada e os fatores externos fossem bem controlados/monitorados. Além disso, era importante saber que poderia contar com a empresa durante os primeiros momentos de adaptação. (Nesse ponto agradeço a presteza dada por Aline, Elton e Gustavo da Agriness. Com o apoio deles, todas as etapas da minha mudança foram extremamente controladas.) Passagem comprada, roupas na mala, partiu Floripa!Considerando todos esses fatores e acontecimentos, sair de Maceió se resumia a quatro ações: me desligar do antigo emprego, me despedir dos familiares e amigos, arrumar as bagagens e embarcar em um voo só de ida para Floripa.Felizmente tudo aconteceu como previsto. O desligamento do antigo emprego ocorreu sem complicações, realizei algumas despedidas (pequenas e para poucos, para evitar gastos :) ), arrumei as bagagens (somente roupas e poucos equipamentos), comprei a passagem e no dia 24 de junho embarquei para Florianópolis…Por enquanto é isso, pessoal! Na próxima postagem irei contar como foi a chegada a nova cidade e início dos trabalhos na Agriness. Também irie contar como foi o início da batalha para encontrar uma moradia.Abraços e até próxima!" }, { "title": "Diário de uma Mudança: Parte 1 - Processo Seletivo", "url": "/posts/diario-de-uma-mudanca-parte-1/", "categories": "general", "tags": "general, personal", "date": "2017-08-02 23:00:11 -0300", "snippet": "Há pouco mais de um mês, saí da minha cidade natal, Maceió/AL, e me mudei definitivamente para Florianópolis/SC. Deixei para trás família, amigos, casa, conforto e antigo emprego para enfrentar um novo desafio.Apesar de já ter passado um bom tempo entre Maceió, São Paulo e Rio de Janeiro, dessa vez é diferente. Dessa vez não poderei voltar para cidade que nasci e que tanto gosto, pelo menos por enquanto. Dessa vez a mudança não será temporária, será definitiva.Com todas essas mudanças, resolvi escrever sobre o processo que me levou a sair do Nordeste e vir para o Sul do país. Tentarei compartilhar com vocês todas as etapas da mudança, desde o processo seletivo até os dias atuais.Para que as postagens não fiquem muito extensas, irei dividi-las em quatro partes: Parte 1 – Processo Seletivo Parte 2 – Passagem Só de Ida Parte 3 – Primeira Semana em Floripa Parte 4 – Lar Doce LarEntão vamos lá!Por que mudar de emprego?Quando decidi aceitar a proposta em Florianópolis, muitas pessoas me perguntaram o porquê de sair de um trabalho considerado estável para me arriscar em outro Estado. Para essa pergunta minha resposta era relativamente simples: meu objetivo era sair da zona de conforto e buscar o crescimento profissional que meu antigo emprego não ofereceria.A falta de perspectiva de crescimento foi um dos principais motivos da busca pela mudança de emprego. Apesar de ter uma equipe excelente e muitos desafios, meu antigo emprego não oferecia muito além disso. Passei dois anos como líder da equipe de desenvolvimento e apenas as responsabilidades cresciam.Considerando o atual cenário do país, a baixa valorização de pessoal é compreensível em alguns pontos. Entretanto, esperar o cenário político se ajustar para ter algum reconhecimento era algo que não podia fazer. Logo, buscar alternativas era a solução. Assim, iniciei a fase de busca por um novo emprego.A busca pelo novo emprego - GeekHunterApós definido que estava na hora de encontrar um novo trabalho, busquei plataformas que ofertassem vagas de emprego para área de TI. Depois várias tentativas, encontrei a plataforma GeekHunter, que acabou sendo a única que funcionava de forma eficiente.A GeekHunter é totalmente voltada para vagas de TI. Nela existem algumas etapas que precisam ser concluídas para que seu currículo fique disponível para as empresas. Basicamente, essas etapas envolvem preenchimento de currículo, teste teórico, teste de programação e gravação de vídeo de apresentação (opcional) - veja melhor como funciona.Após concluída as etapas na GeekHunter, meu currículo estava disponível. A partir daí, participei de alguns processos seletivos – de startups até grandes empresas como Mercado Livre e B2W.O emprego certoDado que a mudança de emprego não necessitava de urgência, realizei vários processos seletivos em busca de uma empresa que se encaixasse no perfil que buscava (e, claro, quisesse me contratar!).Após cerca de quatro meses de muitas entrevistas, testes, derrotas e propostas incompatíveis, iniciei o processo seletivo na empresa Agriness – que até o momento nunca havia ouvido falar.Todo o processo envolveu cerca de quatro etapas, dentre entrevistas e testes – tudo acompanhado pela equipe do GeekHunter. A medida que caminhava pelas etapas da seleção, mais interessado estava pelo desafio que a empresa tinha pela frente. Quando cheguei a última etapa, tinha a certeza que era naquela empresa que gostaria de trabalhar. Felizmente, fui aprovado no processo seletivo e a proposta financeira era compatível com o que procurava.Finalmente havia encontrado o emprego certo, entretanto precisava me mudar para Florianópolis…Bom, por enquanto é isso! Na próxima postagem irei compartilhar como foi a primeira etapa da mudança para Floripa – desligamento do antigo emprego, bagagens, início dos trabalhos, etc.Abraços e até a próxima postagem." }, { "title": "Melhorando suas Habilidades: Práticas de Programação", "url": "/posts/plataformas-aprendizado-treinamento/", "categories": "programming", "tags": "programming, learning", "date": "2017-07-17 18:10:11 -0300", "snippet": "Essa semana, eu e alguns amigos estamos em uma competição para saber quem resolver a maior quantidade de problemas na CodeFights. A partir daí, me veio a ideia de listar algumas plataformas que podem ajudar a praticar programação.Basicamente, eu utilizo duas plataformas: HackerRank: eles oferecem a possibilidade de você realizar treinamentos desde do básico ao avançado em determinadas linguagens (Python, Ruby, Java, C/C++, etc) ou áreas (SQL, IA, Segurança, Programação Funcional, etc.). A medida que os testes são realizados, você vai ganhando “badges”. Nessa plataforma também são realizados testes para seleções. CodeFights: essa é uma das plataformas que mais venho gostando de utilizar. Além de oferecer uma disputa saudável entre amigos, eles oferecem a possibilidade de práticas para entrevistas ou treinamento com bots de grandes empresas. Caso queiram se inscrever, utilizem esse link e ajudem um amigo a ganhar alguns pontos. :) Além dessas, existem muitas outras que não utilizo com tanta frequência. A seguir mais algumas indicações: Codility Exercism (by @tiagocedrim) Codewars (by @tiagocedrim) CheckIO (by @marcelodufe)Bom, pessoal. Espero que essas plataformas possam ajudá-los a melhorar suas habilidades.Abraços e até a próxima! :)" }, { "title": "Servindo Aplicações Django com NGINX + Supervisor + Gunicorn + Virtualenv", "url": "/posts/servindo-aplicacoes-django-com-nginx-supervisor-gunicorn-virtualenv/", "categories": "python", "tags": "python, django, migrations, database", "date": "2017-07-13 19:16:11 -0300", "snippet": "Existem diversas maneiras de disponibilizar aplicações web escritas em Python. Atualmente tenho utilizado bastante a solução que combina o uso do NGINX, Supervisor, Gunicorn e virtualenv.Com essa nova solução, encontrei vantagens que envolvem principalmente performance, manutenção e monitoramento dos serviços.Para entender um pouco mais sobre a configuração dessa solução, devemos entender melhor cada uma dessas ferramentas.NGINXO NGINX (lê-se “engine x”) é um servidor web open source e um server de proxy reverso para protocolos HTTP, SMTP, POP3 e IMAP, com foco em alta concorrência, performance e baixo uso de memória. Por tratar as requisições web como “event-based web server”, o NGINX consome menos memória que o Apache, que é baseado no conceito de “process-based server”.Na solução que utilizaremos, o NGINX servirá somente como servidor de proxy HTTP e reverso.SupervisorO Supervisor é uma ferramenta que permite monitorar e controlar uma série de processos em um sistema operacional UNIX-like. Ele utiliza a arquitetura cliente/servidor.Na solução adotada, o Supervisor tem a responsabilidade de iniciar e manter a aplicação Python sendo executada.GunicornO Gunicorn (Green Unicorn) é um servidor WSGI para UNIX escrito em Python puro. É uma ferramenta portada do projeto Ruby’s Unicorn, que não possui dependências e é de fácil instalação e uso.Na solução, o Gunicorn deverá executar a aplicação Python.VirtualenvComo explicado do post “Virtualenv: Ambiente virtuais isolados em Python”, o virtualenv é uma ferramenta que permite a criação de ambientes virtuais isolados para projetos Python. Ela auxilia na gestão de dependência, garantindo que um projeto não interfira no outro.Configurando ServiçoFeita as apresentações, podemos iniciar a configuração da solução. A arquitetura que montaremos é melhor apresentada na imagem a seguir.Para facilitar a manutenção da aplicação, deveremos organizar os diretórios do projeto da seguinte forma:|-- /var/www/ |-- {projeto} |-- config |-- logs |-- env |-- publicOs diretórios criados tem os seguintes objetivos: config: diretório contendo os arquivos de configuração do projeto. Neste diretório estão contidos os arquivos do Supervisor, Gunicorn e de configuração do projeto. public: diretório contendo todos os arquivos fontes do projeto. env: diretório contendo a virtualenv do projeto. Cada projeto possui sua virtualenv. logs: diretório contendo os arquivos de log do NGINX e Supervisor.Configuração da virtualenvO primeiro passo que realizaremos é a configuração da virtualenv. Sendo assim, deveremos criar uma virtual dentro do diretório “env”. Caso ainda não saiba como fazer, leia a postagem “Virtualenv: Ambiente virtuais isolados em Python”.Posteriormente, instale as dependências do seu projeto utilizando o pip.Configuração do GunicornPara configurar o Gunicorn deveremos primeiro instalá-lo na virtualenv do projeto. Para isso utilize o pip da virtualenv.pip install gunicornAgora deveremos criar o arquivo de configuração do Gunicorn dentro do diretório “config”. Iremos criar o arquivo “gunicorn_config.conf”. Nesse arquivo deveremos adicionar o seguinte trecho de código:command = '/var/www/{projeto}/env/{nome_da_virtualenv}/bin/gunicorn'pythonpath = '/var/www/{projeto}/env/{nome_da_virtualenv}/'bind = '{endereco_porta_configurado_gunicorn}'workers = 2Em resumo, o trecho acima indica o comando que deverá ser executado, o diretório do Python, o endereço do socket e o número de processos que deverão ser utilizados. Esse arquivo será utilizado pelo Supervisor.Configuração do SupervisorPara configurar o Supervisor deveremos criar o arquivo “supervisor.conf” no diretório “config” e adicionar o seguinte trecho:[program:{projeto}]command=/var/www/{projeto}/env/{nome_da_virtualenv}/bin/gunicorn -c /var/www/{projeto}/config/gunicorn_config.py {wsgi_do_projeto_django}:applicationdirectory=/var/www/{projeto}/public/stderr_logfile=/var/www/{projeto}/logs/long.err.logstdout_logfile=/var/www/{projeto}/logs/long.out.logO trecho acima indica ao Supervisor qual commando será executado, o diretório onde ele será executado e a localização dos arquivos de log/out. Percebam que o comando a ser executado é referente ao Gunicorn.Após criar o arquivo de configuração do Supervisor, devemos habilitá-lo. Para isso, deveremos adicionar o caminho do arquivo na seção “include” em “/etc/supervisor/supervisor.conf”. Posteriormente reinicie o serviço Supervisor.[include]files = /var/www/{projeto}/config/supervisor.confConfiguração do NGINXPor fim, deveremos configurar o NGINX. Ele funcionará somente como um proxy reverso. server { server_name www.dominio.com; access_log /var/www/{projeto}/logs/access.log; error_log /var/www/{projeto}/logs/error.log; location / { proxy_pass_header Server; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_redirect off; proxy_connect_timeout 10; proxy_read_timeout 10; proxy_pass {endereco_porta_configurado_gunicorn}; add_header P3P 'CP=\"ALL DSP COR PSAa PSDa OUR NOR ONL UNI COM NAV\"'; } }O trecho acima deverá ser adicionado a um arquivo dentro do diretório do NGINX, “/etc/nginx/sites-available/”.Logo após deveremos habilitar a configuração criando um link simbólico em “/etc/nginx/sites-enable/” para o arquivo que acabamos criar (ou simplesmente adicionar o arquivo criado diretamente no diretório“/etc/nginx/sites-enable/” - mas não recomendo).ln -s /etc/nginx/sites-available/{projeto}.conf {projeto}.confNa configura do NGINX existe uma série de configurações de proxy, tentarei comentar sobre essasconfigurações em um outro post.Bom, é isso ai pessoal! Com as ferramentas e configurações apresentadas o serviço deverá está disponível.Abraços e até a próxima!Referências https://virtualenv.pypa.io/en/stable/ http://supervisord.org/ http://gunicorn.org/ https://www.nginx.com/ https://www.digitalocean.com/community/tutorials/apache-vs-nginx-practical-considerations (Valeu @nandooliveira) https://www.nginx.com/resources/library/complete-nginx-cookbook/?utm_source=twitter&amp;utm_medium=cpc&amp;utm_campaign=core (Valeu @tiagocedrim)" }, { "title": "Django e Migrations: Manipulação de Base de Dados", "url": "/posts/django-e-migrations-manipulacao-de-base-de-dados/", "categories": "python", "tags": "python, django, migrations, database", "date": "2017-07-11 20:14:11 -0300", "snippet": "Criar, alterar ou remover as estruturas ou dados de um banco de dados manualmente sempre foi uma atividade de alto risco e demorada. Mesmo com a realização de backups, uma alteração incorreta em uma base de dados (de produção!) pode trazer muita dor de cabeça para a equipe de desenvolvimento e infraestrutura.Percebendo essa dificuldade, a partir do Django 1.7 foi inserida a migração de forma nativa. Antes disso existiam soluções de terceiros, como o South, base da atual implementação nativa.SouthSouth é um projeto que iniciou em 2007 e que em breve deverá ser descontinuado. Como dito na própria documentação do projeto, o último grande lançamento foi a versão 1.0. A partir daí somente versões com atualizações de segurança foram/serão lançadas.Atualmente o autor da biblioteca concentra suas atenções na migração nativa do Django, que utiliza como base o South.Migração Nativa (Migrations)No Django, existem vários comandos que podem ser utilizados para interagir com a base de dados. Abaixo citamos os principais utilizados: migrate: possibilita aplicar ou cancelar migrations. Também possibilita listar o status das operações. makemigrations: responsável pela criação de novos migrations baseados nas alterações dos modelos. sqlmigrate: exibi as SQLs de um migrations. showmigrations: apresenta todos os migrations associados ao projeto.Em resumo, podemos dizer que o makemigrations é o responsável por criar versões das alterações realizadas nos modelos das aplicações, enquanto que o migrate é o responsável pela aplicação de tais mudanças na base de dados.Parâmetros EspeciaisAlguns dos comandos acima aceitam parâmetros que podem modificar a ação a ser executada. Abaixo citaremos alguns deles.makemigrationsAlguns dos parâmetros aceitos no makemigrations são: --name &lt;nome_da_alteracao&gt;: através desse parâmetro é possível nomear a alteração realizada. --empty &lt;nome_do_app&gt;: indicar que um arquivo de migração vazio deve ser criado (isso geralmente é (utilizado para realizar alterações nos dados – data migrations. Poderemos ver isso em outro post!). Com esse parâmetro, o Django irá colocar o arquivo de migração vazio no lugar certo, sugerir um nome e adicionar dependências. --merge: esse parâmetro é utilizado para resolver conflitos simples de migrations. Em casos de conflitos mais complexos o Django provavelmente não realizará o merge e outro método deverá ser utilizado.migrateJá no migrate, alguns dos parâmetros aceitos são: --fake: nesse caso o Django somente irá marcar o migrations como executado, entretanto não realizará alterações na base dados. --fake-initial: em resumo, esse parâmetro irá fazer com que o Django verifique se as tabelas existem e, em caso afirmativo, irá falso-aplicar migrações iniciais. Ou seja, o Django irá marcar os migrations de criação da tabela como já executado. Do mesmo modo com as colunas das tabelas. --list: irá mostrar uma lista de todas as migrations conhecidas e quais foram aplicadas.Bom pessoal, espero ter ajudado. Em outras postagens pretendo abordar outros assuntos relacionados ao Migrations (data migrations, rollback, outros métodos de merge, etc).Abraços e até a próxima!Links Interessantes Documentação Oficial (Django 1.9) Documentação do South" }, { "title": "Virtualenv: Ambiente virtuais isolados em Python", "url": "/posts/virtualenv-ambiente-virtuais-isolados-em-python/", "categories": "python", "tags": "python, virtualenv", "date": "2017-07-11 18:15:11 -0300", "snippet": "Um problema constante para desenvolvedores que estão envolvidos em vários projetos é a configuração do ambiente de desenvolvimento. É bastante comum projetos possuírem dependência da mesma biblioteca, porém com versões diferentes.Além desse problema, existem ambientes em que não temos permissão de instalar pacotes no sistema. Ou, que queremos proteger nossas configurações do sistema das configurações dos projetos.O que fazer nesses casos? Como manter ambientes diferentes na mesma máquina? Como isolar um projeto do outro?Para desenvolvedores Python, esse problema é facilmente resolvido com o uso da virtualenv.O que é virtualenv?Como dita na própria documentação, virtualenv é uma ferramenta para criação de ambientes Python isolados. Ela oferece uma maneira simples de instalar pacotes Python independentes do sistema global. Além de evitar o conflito com pacotes instalados.Como trabalhar com virtualenv?1º Passo: InstalaçãoA instalação da virtualenv é extremamente prática e simples (no ambiente Linux). Para começo, precisamos instalar o gerenciador de pacotes pip (comando testado no Ubuntu).sudo apt-get install python-pip python-dev build-essentialPara versões do Ubuntu anteriores a 10.10, o procedimento muda um pouco e pode ser visto aqui.Feita a instalação do pip, podemos instalar a virtualenv.blog@marcospereira:~/$ sudo pip install --upgrade virtualenv2º Passo: Criação e Ativação de Ambientes VirtuaisCom a virtualenv instalada, agora podemos começar a criar os ambientes virtuais para projetos Python. Vamos criar um ambiente:blog@marcospereira:~/$ virtualenv AmbienteVirtualApós a execução do comando acima, será criado um novo diretório contendo a estrutura do Python (sites-packages, bin, include, etc). Podemos carregar o ambiente criado executando o arquivo activate que fica dentro do diretório bin da nova estrutura criada.blog@marcospereira:~/$ source AmbienteVirtual/bin/activate(AmbienteVirtual) blog@marcospereira:~/$A partir daí você estará trabalhando no ambiente criado e o prompt do seu shell indicará qual é este ambiente. No nosso caso, é o AmbienteVirtual.3º Passo: Instalação de PacotesEstando o ambiente virtual ativado, todas as instalações de pacotes Python afetarão somente esse ambiente. Sendo assim, para instalar um pacote, basta utilizar o pip.(AmbienteVirtual) blog@marcospereira:~/$ pip install NomeDoPacoteComo Organizar?Bem, agora que você já sabe configurar um ambiente virtual, existem algumas práticas que costumo adotar:Instale somente os pacotes essenciais ao projeto. Muitas vezes instalamos pacotes que deixamos de utilizar. Isso pode dar uma grande dor de cabeça na hora de verificar as dependências do seu projeto.Então, sempre monitore os pacotes instalados no seu ambiente. Para isso você pode utilizar o comando:(AmbienteVirtual) blog@marcospereira:~/$ pip freezePara direcionar a saída para um arquivo, utilize:(AmbienteVirtual) blog@marcospereira:~/$ pip freeze &gt; requirements.txtEsse comando gera uma lista de pacotes instalados e suas versões. Em geral, adicionamos ao projeto o arquivo requirements.txt com essa listagem para que outros desenvolvedores saibam quais pacotes o projeto necessita.Para instalar os pacotes a partir de um arquivo requirements.txt, utilize o comando:(AmbienteVirtual) blog@marcospereira:~/$ pip install -r requirements.txtCom vários ambientes virtuais instalados, você pode facilitar sua vida através da extensão virtualenvwrapper. Alguns das funcionalidades do virtualenvwrapper são: Organiza todos os ambientes virtuais em um único lugar. Wrappers para gerenciar ambientes virtuais (criar, apagar, copiar). Usar um único comando para alternar entre ambientes. Reconhecimento de comandos para trabalhar com ambientes.Bom galera, acho que é isso! Espero ter ajudado.Abraços a todos e até a próxima." } ]
